{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "\n",
    "#####################################################\n",
    "## This is the example code of the pytorch tutorial\n",
    "## of the deep learning lab. Ich you want to\n",
    "## visualize the inputs, uncomment the lines 94-99.\n",
    "## Data will be automatically downloaded to ../data/\n",
    "## Author: Johan Vertens\n",
    "#####################################################\n",
    "\n",
    "\n",
    "# Here we use predefined datasets from 'torchvision'\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=128, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.340212\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.326772\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.257401\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.258569\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.169802\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.156617\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.040260\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.890446\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.661392\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.531009\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.453621\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.233948\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.165544\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.008506\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.042789\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.065648\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.928448\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.800215\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.769888\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.904163\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.866244\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.739073\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.753225\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.725561\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.799246\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.827751\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.740456\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.864822\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.603343\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.476920\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.600499\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.702226\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.490603\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.580593\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.510032\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.570604\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.529580\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.542043\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.603862\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.696669\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.509412\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.526589\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.529289\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.806209\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.531491\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.466317\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.710650\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definition of the network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, padding=2)    # FYI: In the lecture I forgot to add the padding, thats why the feature size calculation was wrong\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, padding=2)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(980, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 980)   # Flatten data for fully connected layer. Input size is 28*28, we have 2 pooling layers so we pool the spatial size down to 7*7. With 20 feature maps as the output of the previous conv we have in total 7x7x20 = 980 features.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# We create the network, shift it on the GPU and define a optimizer on its parameters\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# This function trains the neural network for one epoch\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move the input and target data on the GPU\n",
    "\n",
    "        data, target = data.to(device), target.to(device)  # .cuda() works too instead of .to(device)\n",
    "        # Zero out gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass of the neural net\n",
    "        output = model(data)\n",
    "\n",
    "        # Calculation of the loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # Backward pass (gradient computation)\n",
    "        loss.backward()\n",
    "        # Adjusting the parameters according to the loss function\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)    \n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            '''\n",
    "            if not pred.eq(target.view_as(pred)):   ## If you just want so see the failing examples\n",
    "                cv_mat = data.cpu().data.squeeze().numpy()\n",
    "                cv_mat = cv2.resize(cv_mat, (400, 400))\n",
    "                cv2.imshow(\"test image\", cv_mat)\n",
    "                print(\"Target label is : %d\" % target.cpu().item())\n",
    "                print(\"Predicted label is : %d\" % (pred.cpu().data.item()))\n",
    "                cv2.waitKey()\n",
    "            '''\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            print(correct)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "num_train_epochs = 1\n",
    "for epoch in range(1, num_train_epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
